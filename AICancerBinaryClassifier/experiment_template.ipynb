{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Experiment\n",
    "This notebook serves to explore the provided dataset, as well as introduce options for neural network architecture and the training process.\n",
    "\n",
    "I have tried to structure individual components in a way where it's easy for you to reuse the functions in your own modules, should you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup\n",
    "This notebook comes with a `requirements.txt` file containing dependencies for you to install.\n",
    "It was run on Python 3.10.6; dependencies will be forward compatible but some of the typing features used in the code provided here require **at least** Python 3.10, so the code is not backward compatible.\n",
    "\n",
    "To set up an environment to run these experiments (assuming your operating system is Windows), run through the following steps:\n",
    "\n",
    "1. Create a Python environment.\n",
    "```bash\n",
    "python -m venv venv\n",
    "```\n",
    "\n",
    "2. Activate the environment and install dependencies.\n",
    "```bash\n",
    "venv\\Scripts\\activate\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "A side note: Windows may ~~bitch and moan about~~ deny access to the provided script without you modifying privileges. If so, follow the instructions provided in the terminal. Thanks, Windows.\n",
    "\n",
    "3. Have fun with the Jupyter notebook. If you have questions about Jupyter, let me know.\n",
    "\n",
    "> A quick side note: this setup will download the GPU-enabled binaries for PyTorch, which requires you to have CUDA set up (and a CUDA-enabled GPU in your rig, which I think you do, you have an RTX3060 right?).\n",
    "> You can either set up CUDA on your machine or not bother; the code will pick the best device available, so it will also run on CPU. It will be significantly slower but since we're working with fairly simple data, I don't anticipate that that will be an issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Exploring the Data\n",
    "In this section, we will load all the data into memory and explore it a bit.\n",
    "I will also create an artifact in persistent storage that you can use moving forward so you don't have to apply the same preprocessing steps every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from uuid import UUID\n",
    "from os import walk, path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Note that I have configured the logger not to print debug statements while actually using some.\n",
    "# If you want to debug further, set to `logging.DEBUG`.\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "LOGGER = logging.getLogger(\"experiment_notebook\")\n",
    "ARTIFACT_NAME = \"complete_data.csv\"\n",
    "\n",
    "\n",
    "def load_data(\n",
    "    root: str,\n",
    "    dir_positives: str,\n",
    "    dir_negatives: str,\n",
    "    cache: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load data from the provided directories and stores the compiled dataframe in your file system.\n",
    "    If such a cache is present, the function will load from it instead.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(path.join(root, ARTIFACT_NAME))\n",
    "        LOGGER.info(\"Loaded dataset from cache.\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        LOGGER.info(\n",
    "            \"Could not find dataset in cache, will attempt to build from scratch.\"\n",
    "        )\n",
    "    positives = read_raw_data_from_directory(path.join(root, dir_positives))\n",
    "    positives[\"cancer\"] = 1\n",
    "    negatives = read_raw_data_from_directory(path.join(root, dir_negatives))\n",
    "    negatives[\"cancer\"] = 0\n",
    "    data = pd.concat([positives, negatives])\n",
    "    LOGGER.info(\"Finished building dataset from scratch.\")\n",
    "    if cache:\n",
    "        dataset_filepath = path.join(root, ARTIFACT_NAME)\n",
    "        if not path.exists(dataset_filepath):\n",
    "            LOGGER.info(\"Persisting dataset in '%s'.\", dataset_filepath)\n",
    "            data.to_csv(dataset_filepath, index=None)\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_raw_data_from_directory(dirname: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convenience function for traversing all subdirectories from the provided starting directory,\n",
    "    loading all relevant files into memory, and into a usable format.\n",
    "\n",
    "    @Josh: You could conceivably use the filepath as the index (or just add it as a column) so you\n",
    "    can later map individual rows in your dataset onto the source datasets. I just didn't bother\n",
    "    because I didn't really see a value here.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dirname : str\n",
    "        The name of the directory (absolute or relative) containing data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A single DataFrame containing all data from the directory.\n",
    "    \"\"\"\n",
    "    LOGGER.info(\"Walking data directory %s and reading in files...\", dirname)\n",
    "    rows = []\n",
    "    for root, _, files in tqdm(\n",
    "        walk(\"data/miRNA Files - Lung Cancer\", topdown=False)\n",
    "    ):\n",
    "        for filename in files:\n",
    "            # Ugly workaround for identifying UUID because I couldn't be FUCKED to write a regex rn.\n",
    "            try:\n",
    "                _ = UUID(filename.split(\".\")[0])\n",
    "            except ValueError:\n",
    "                LOGGER.debug(\n",
    "                    \"Skipped file because it did not start with a UUID-like string: %s\",\n",
    "                    filename,\n",
    "                )\n",
    "                continue\n",
    "            rows.append(read_txt_data_file(root, filename))\n",
    "    data = pd.DataFrame(rows).reset_index()\n",
    "    # Drop the index column that no longer serves as the index because pandas be funky.\n",
    "    del data[\"index\"]\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_txt_data_file(directory: str, filename: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Read a miRNA file into memory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory : str\n",
    "        The directory that contains the file. Could be nested, so this string could be multiple and\n",
    "        it always contains the root directory.\n",
    "    filename : str\n",
    "        The name of the file to be loaded, including file ending.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A series of floats where each index corresponds to a MiRNA ID.\n",
    "    \"\"\"\n",
    "    filepath = path.join(directory, filename)\n",
    "    LOGGER.debug(\"Loading data from file %s\", filepath)\n",
    "    # We assume consistently tabular style in all txt files. This is technically a bad assumption to\n",
    "    # make but I'm not writing code for a product.\n",
    "    data = pd.read_csv(filepath, sep=\"\\t\", index_col=\"miRNA_ID\")\n",
    "    # We can drop all irrelevant data to return a simple feature vector.\n",
    "    return data[\"reads_per_million_miRNA_mapped\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that we will use lowercase variable names in the following but we are creating variables in the global namespace.\n",
    "\n",
    "**PEP8 dictates that global variable names are uppercase in Python.** We are not doing that here because Jupyter notebooks are often treated a little differently.\n",
    "\n",
    "I encourage you to be mindful of this fact regardless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:experiment_notebook:Loaded dataset from cache.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cancer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [cancer]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_data(\n",
    "    root=\"Databases\",\n",
    "    dir_positives=\"miRNA Files - Lung Cancer\",\n",
    "    dir_negatives=\"miRNA Files - Normal\",\n",
    ")\n",
    "\n",
    "# Let us look at what the data looks like.\n",
    "data.head()\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now go a little deeper into the data at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains a total of 0 samples.\n",
      "If you use 20% of your data for validation, this will leave you with 0 training samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63640/523393987.py:9: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  num_negatives = label_occurrences[0]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m label_occurrences \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcancer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you use 20\u001b[39m\u001b[38;5;132;01m% o\u001b[39;00m\u001b[38;5;124mf your data for validation, this will leave you with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.8\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m training samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m num_negatives \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_occurrences\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_negatives\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of the samples (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(num_negatives\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%) are negative.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m num_positives \u001b[38;5;241m=\u001b[39m label_occurrences[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/series.py:1118\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(key) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_should_fallback_to_positional:\n\u001b[1;32m   1109\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1110\u001b[0m         \u001b[38;5;66;03m# GH#50617\u001b[39;00m\n\u001b[1;32m   1111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries.__getitem__ treating keys as positions is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m   1117\u001b[0m     )\n\u001b[0;32m-> 1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset contains a total of {len(data)} samples.\")\n",
    "label_occurrences = data[\"cancer\"].value_counts()\n",
    "\n",
    "print(\n",
    "    \"If you use 20% of your data for validation, this will leave you with \"\n",
    "    f\"{round(len(data)*0.8)} training samples.\"\n",
    ")\n",
    "\n",
    "num_negatives = label_occurrences[0]\n",
    "print(\n",
    "    f\"{num_negatives} of the samples ({round(num_negatives/len(data)*100, 2)}%) are negative.\"\n",
    ")\n",
    "\n",
    "num_positives = label_occurrences[1]\n",
    "print(\n",
    "    f\"{num_positives} of the samples ({round(num_positives/len(data)*100, 2)}%) are positive.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we have a perfectly balanced dataset. This is neat! That means we don't need to bother with fancy sampling techniques, class weights or sample weights.\n",
    "\n",
    "Unfortunately, 3576 samples is... not a lot.\n",
    "\n",
    "Next, let's look at the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [col for col in data if col.startswith(\"hsa\")]\n",
    "\n",
    "print(\n",
    "    f\"Our input feature vector will likely be {len(feature_cols)}-dimensional.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh-oh. That's... a lot of features. Especially given the size of our dataset. Let's look if we have columns that are \"dead\", meaning they have no variance. These columns carry no information for the classifier to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dead_cols = []\n",
    "for col in feature_cols:\n",
    "    std = data[col].std()\n",
    "    if std == 0:\n",
    "        dead_cols.append(col)\n",
    "print(f\"{len(dead_cols)} columns are dead!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is good because it reduces the feature space but it still leaves us with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_cols) - len(dead_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature columns. That's still a lot!\n",
    "\n",
    "## 2.1 Next Steps\n",
    "What are our options? Well, you have a few. You could:\n",
    "\n",
    "\n",
    "### 2.1.1 Just Throw It All In And See What Happens\n",
    "Honestly, why the fuck not. If it performs like shit, you can still follow a different approach.\n",
    "\n",
    "### 2.1.2 Do Feature Selection\n",
    "Considering the ratio of dimensions to samples, your classifier is unlikely to properly cut through the noise (assuming that some miRNA IDs carry no indicative value for cancer). You could identify the correlation coefficient between each individual feature and the label, and set something like a \"dead zone\" where you kick out values that do not correlate at all with your target label.\n",
    "\n",
    "Be mindful, though, as it may be that there _is_ an underlying correlation between a certain combination of miRNA ID values. You would obviously be missing that in your preliminary analysis and potentially remove meaningful data.\n",
    "\n",
    "### 2.1.3 Do Dimensionality Reduction\n",
    "Technically, neural networks already kinda sorta do this. But you _could_ do some of it yourself as a preprocessing step and then use the resulting data in your classification problem.\n",
    "\n",
    "### 2.1.4 Do Fancy Architectures\n",
    "A [cursory glance at relevant literature in the biomedical space](https://www.nature.com/articles/s42256-023-00744-z) suggests a bunch of options you could follow already. If there is an actual spatial component to the individual miRNA IDs (like, some sort of relative distance to one another) you could, for example, consider a convolutional approach.\n",
    "There is some intersection with `2.1.3` but whateverrrrrr.\n",
    "\n",
    "### Final Words\n",
    "I am very tired. Please excuse any inaccuracies that might be in this bitch. I hope you've found it instructive and I would suggest you just try option `2.1.1` at first and work forward from there."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
