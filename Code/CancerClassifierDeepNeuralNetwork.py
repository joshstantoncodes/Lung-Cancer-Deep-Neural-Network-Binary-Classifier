"""  Lung Cancer Binary Classifier using miRNA Expression

    This program reads in a parquet file and trains a deep neural network
    using the previous declarations in the parquet to predict cancer from
    noncancer based on the expression of miRNA in the file. This is applied
    in three batches: Batch #1 is nonspecific and tests the entire dataset
    of miRNA with no selection; batch #2 focuses on the miRNA highlighted and
    found to have increased expression in lung cancer patients as per Stanton
    et al. (2021); batch #3 investigates prediction capabilities based on
    a hypothesis generated by Stanton et al (2021) regarding the expression of
    miRNA as "families" being affected by lung cancer as a group rather than
    primarily individually.

    Author: Josh Stanton
    Date: November 29th, 2024
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import tensorflow as tf
from tensorflow.python.keras.backend import epsilon

# Load the data
data = pd.read_parquet('complete_data.parquet')

"""  TRAINING BATCH #1

    The first set of training simply puts the entire set of data into the training model 
    and assesses every single miRNA across the board. This should have lower accuracy due 
    to the wide ranging variance in the datasets. 
"""
X = data.drop(['cancer'], axis=1)  # Remove 'cancer' column to allow learning
Y = data['cancer']  # Indicates the correct dissertation between cancer and noncancer

# Data pre-processing
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)  # Calculates the mean and standard deviation
# of each miRNA

# Split into training and testing sets of even distribution
X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)

""" Deep Neural Network Model Building 

    Create the deep neural network model, beginning with 256 nodes and gradually training
    layers with fewer nodes, ultimately resulting in the binary classification done using the
    sigmoid node.
"""
model = tf.keras.Sequential([
    tf.keras.layers.Dense(256, activation='relu', input_shape=(X.shape[1],)),
    tf.keras.layers.BatchNormalization(
        momentum=0.5,
        epsilon=0.00001,
        center=True,
        scale=True,
        beta_initializer='zeros',
        gamma_initializer='ones'
    ),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.BatchNormalization(
        momentum=0.5,
        epsilon=0.00001,
        center=True,
        scale=True,
        beta_initializer='zeros',
        gamma_initializer='ones'
    ),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.BatchNormalization(
        momentum=0.5,
        epsilon=0.00001,
        center=True,
        scale=True,
        beta_initializer='zeros',
        gamma_initializer='ones'
    ),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

""" Compiles the model:
   
    The algorithm uses the adam optimizer for gradient descent,
    using the binary_crossentropy as a loss function and uses 
    prediction accuracy as the guiding performance metric. 
"""
model.compile(optimizer='adamax', loss='binary_crossentropy', metrics=['accuracy'])

""" Run the TensorFlow Training Function:
    
    The model is trained using the predefined testing sets of the labelled and
    unlabeled data, running for 200 epochs (iterations), with a batch size of 32
    specifying that every 32 samples leads to an update in learning. The validation
    split is also 50% to match the distribution of the previous split of test and 
    training data.  
"""
history = model.fit(X_train, Y_train, epochs=50, batch_size=32, validation_split=0.2)

# Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, Y_test)
print(f"Test accuracy: {test_accuracy}")

# Make predictions
predictions = model.predict(X_test)

"""  TRAINING BATCH #2

    This batch of training focuses on the miRNA identified in my thesis as established 
    miRNA of interest, yielding higher expression in lung cancer patients versus noncancerous patients.
"""

#
selected_mirnas = ['hsa-mir-181c', 'hsa-mir-500a', 'hsa-mir-99a', 'hsa-mir-10b']
X = data[selected_mirnas]
Y= data['cancer']

# Preprocess the data
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Split into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)
""" Deep Neural Network Model Building 

    Create the deep neural network model, beginning with 256 nodes and gradually training
    layers with fewer nodes, ultimately resulting in the binary classification done using the
    sigmoid node.
"""
model = tf.keras.Sequential([
    tf.keras.layers.Dense(256, activation='relu', input_shape=(X.shape[1],)),
    tf.keras.layers.BatchNormalization(
        momentum=0.5,
        epsilon=0.00001,
        center=True,
        scale=True,
        beta_initializer='zeros',
        gamma_initializer='ones'
    ),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.BatchNormalization(
        momentum=0.5,
        epsilon=0.00001,
        center=True,
        scale=True,
        beta_initializer='zeros',
        gamma_initializer='ones'
    ),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.BatchNormalization(
        momentum=0.5,
        epsilon=0.00001,
        center=True,
        scale=True,
        beta_initializer='zeros',
        gamma_initializer='ones'
    ),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
""" Compiles the model:

    The algorithm uses the adamax optimizer for gradient descent,
    using the binary_crossentropy as a loss function and uses 
    prediction accuracy as the guiding performance metric. 
"""
model.compile(optimizer='adamax', loss='binary_crossentropy', metrics=['accuracy'])

""" Run the TensorFlow Training Function:

    the model is trained using the predefined testing sets of the labelled and
    unlabeled data, running for 200 epochs (iterations), with a batch size of 32
    specifying that every 32 samples leads to an update in learning. The validation
    split is also 50% to match the distribution of the previous split of test and 
    training data.  
"""
history = model.fit(X_train, Y_train, epochs=50, batch_size=32, validation_split=0.2)

# Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, Y_test)
print(f"Test accuracy: {test_accuracy}")

# Make predictions
predictions = model.predict(X_test)

"""  TRAINING BATCH #3

    This batch of training focuses on exploring the miRNA "families" hypothesis
    that was generated at the end of the master's thesis, positing that perhaps
    miRNA dysregulation in cancer is perhaps affecting miRNA "families" (miRNA
    that have the same ID number but different suffixes). 
"""

family_mirnas = ['hsa-let-7a-1', 'hsa-let-7a-2', 'hsa-let-7a-3', 'hsa-let-7b', 'hsa-let-7c', 'hsa-let-7d', 'hsa-let-7e',
                 'hsa-let-7f-1', 'hsa-let-7f-2', 'hsa-let-7g', 'hsa-let-7i', 'hsa-mir-130a', 'hsa-mir-130b',
                 'hsa-mir-30a', 'hsa-mir-30b', 'hsa-mir-30c-1', 'hsa-mir-30c-2', 'hsa-mir-30d', 'hsa-mir-30e',
                 'hsa-mir-323a', 'hsa-mir-323b', 'hsa-mir-181a-1', 'hsa-mir-181a-2', 'hsa-mir-181b-1',
                 'hsa-mir-181b-2', 'hsa-mir-181c', 'hsa-mir-181d', 'hsa-mir-500a', 'hsa-mir-500b', 'hsa-mir-10a',
                 'hsa-mir-10b', 'hsa-mir-99a', 'hsa-mir-99b']

X = data[family_mirnas]
Y= data['cancer']

# Preprocess the data
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Split into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)

""" Deep Neural Network Model Building 

    Create the deep neural network model, beginning with 256 nodes and gradually training
    layers with fewer nodes, ultimately resulting in the binary classification done using the
    sigmoid node.
"""
model = tf.keras.Sequential([
    tf.keras.layers.Dense(256, activation='relu', input_shape=(X.shape[1],)),
    tf.keras.layers.BatchNormalization(
        momentum=0.5,
        epsilon=0.00001,
        center=True,
        scale=True,
        beta_initializer='zeros',
        gamma_initializer='ones'
    ),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.BatchNormalization(
        momentum=0.5,
        epsilon=0.00001,
        center=True,
        scale=True,
        beta_initializer='zeros',
        gamma_initializer='ones'
    ),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.BatchNormalization(
        momentum=0.5,
        epsilon=0.00001,
        center=True,
        scale=True,
        beta_initializer='zeros',
        gamma_initializer='ones'
    ),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
""" Compiles the model:

    The algorithm uses the adamax optimizer for gradient descent,
    using the binary_crossentropy as a loss function and uses 
    prediction accuracy as the guiding performance metric. 
"""
# Compile the model
model.compile(optimizer='adamax', loss='binary_crossentropy', metrics=['accuracy'])

""" Run the TensorFlow Training Function:

    The model is trained using the predefined testing sets of the labelled and
    unlabeled data, running for 200 epochs (iterations), with a batch size of 32
    specifying that every 32 samples leads to an update in learning. The validation
    split is also 50% to match the distribution of the previous split of test and 
    training data.  
"""

history = model.fit(X_train, Y_train, epochs=50, batch_size=32, validation_split=0.2)

# Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, Y_test)
print(f"Test accuracy: {test_accuracy}")

# Make predictions
predictions = model.predict(X_test)
